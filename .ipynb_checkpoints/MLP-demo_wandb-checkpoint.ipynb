{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002ce671",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import packages'''\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "import os.path\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import wandb ##weight and bias\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e015e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(DATA_PATH, batch_size):\n",
    "    ## for training\n",
    "    rotation = 15\n",
    "    train_trans = transforms.Compose([transforms.RandomRotation(rotation),\\\n",
    "                                      transforms.RandomHorizontalFlip(),\\\n",
    "                                      transforms.ToTensor(),\\\n",
    "                                      transforms.Normalize((0.5), (0.5))])\n",
    "    train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, download=True,\\\n",
    "                                               train=True, transform=train_trans)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\\\n",
    "                              shuffle=True, num_workers=0)\n",
    "    ## for testing\n",
    "    test_trans = transforms.Compose([transforms.ToTensor(),\\\n",
    "                                     transforms.Normalize((0.5), (0.5))])\n",
    "    test_dataset = torchvision.datasets.MNIST(root=DATA_PATH,\\\n",
    "                                              download=True, train=False, transform=test_trans)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,\\\n",
    "                             shuffle=False, num_workers=0)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "101b7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Fun: write the MLP model'''\n",
    "class MLPModel(nn.Module):\n",
    "    \"\"\"docstring for ClassName\"\"\"\n",
    "    def __init__(self,):\n",
    "        super(MLPModel, self).__init__()\n",
    "        ##-----------------------------------------------------------\n",
    "        ## define the model architecture here\n",
    "        ## MNIST image input size batch * 28 * 28 (one input channel)\n",
    "        ##-----------------------------------------------------------\n",
    "        \n",
    "        ## Write code about three MLP layers below\n",
    "        self.mlp = nn.Sequential(nn.Linear(28*28,100),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.2),\n",
    "                                nn.Linear(100,50),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(50,10)\n",
    "                                )\n",
    "    '''feed features to the model'''\n",
    "    def forward(self, x):\n",
    "        ## write flatten tensor code below [I have done it]\n",
    "        x = torch.flatten(x,1)\n",
    "        ## ---------------------------------------------------\n",
    "        ## write code about MLP predict results\n",
    "        ## ---------------------------------------------------\n",
    "        result = self.mlp(x)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cede10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute accuracy of training and testing\n",
    "def _compute_counts(y_pred, y_batch, mode='train'):\n",
    "    return (y_pred==y_batch).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdf05b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(learning_rate, optimizer, epoch, decay):\n",
    "    \"\"\"initial LR decayed by 1/10 every args.lr epochs\"\"\"\n",
    "    lr = learning_rate\n",
    "    if (epoch > 5):\n",
    "        lr = 0.001\n",
    "    if (epoch >= 10):\n",
    "        lr = 0.0001\n",
    "    if (epoch > 20):\n",
    "        lr = 0.00001\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b512ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_checkpoint(ckp_path, model, epoch, optimizer, global_step):\n",
    "    ## save checkpoint to ckp_path: 'checkpoint/step_100.pt'\n",
    "    ckp_path = ckp_path + 'ckp_{}.pt'.format(epoch+1) \n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'global_step': global_step,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34ff249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ## choose cpu or gpu\n",
    "    seed = 1\n",
    "    torch.manual_seed(seed)\n",
    "    ## numpy.rand(1), 1.1\n",
    "    ## choose GPU id\n",
    "    gpu_id = 0  ## 1, 2, 3,4\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda', gpu_id)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print(\"device: \", device)\n",
    "    ## random seed for cuda\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(72)\n",
    "    \n",
    "    ## initialize hyper-parameters\n",
    "    num_epoches = 10\n",
    "    decay = 0.01\n",
    "    learning_rate = 0.0001\n",
    "    batch_size = 50 #100\n",
    "    ckp_path = 'checkpoint/'\n",
    "    \n",
    "    ## step 1: Data loader to load MNIST data\n",
    "    DATA_PATH = \"./data/\"\n",
    "    train_loader, test_loader=_load_data(DATA_PATH, batch_size)\n",
    "    ##-------------------------------------------------------\n",
    "    ## Step 2: load the MLP model in model.py file\n",
    "    ##-------------------------------------------------------\n",
    "    model =  MLPModel()\n",
    "    ## load model to gpu or cpu\n",
    "    model.to(device)\n",
    "    \n",
    "    ## --------------------------------------------------\n",
    "    ## Step 3: define the Opimization method and LOSS FUNCTION: cross-entropy\n",
    "    ## --------------------------------------------------\n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)  ## optimizer\n",
    "    loss_fun = nn.CrossEntropyLoss()    ## cross entropy loss\n",
    "    \n",
    "    ## ---------------------------------------\n",
    "    ## load checkpoint below\n",
    "    ## ---------------------------------------\n",
    "    \n",
    "    ##  model training\n",
    "    iteration = 0\n",
    "    if True:\n",
    "        model = model.train() ## model training\n",
    "        for epoch in range(num_epoches): #10-50\n",
    "            ## learning rate\n",
    "            adjust_learning_rate(learning_rate, optimizer, epoch, decay)\n",
    "            for batch_id, (x_batch,y_labels) in enumerate(train_loader):\n",
    "                \n",
    "                iteration += 1\n",
    "                x_batch,y_labels = Variable(x_batch).to(device), Variable(y_labels).to(device)\n",
    "                \n",
    "                ## feed input data x into model\n",
    "                output_y = model(x_batch)\n",
    "                ##--------------------------------------------------------------\n",
    "                ## Step 4: compute loss between ground truth and predicted result\n",
    "                ##---------------------------------------------------------------\n",
    "                loss = loss_fun(output_y, y_labels)\n",
    "                \n",
    "                ##----------------------------------------------\n",
    "                ## Step 5: write back propagation steps below\n",
    "                ##----------------------------------------------\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step() # update params\n",
    "                \n",
    "                ##---------------------------------------------------------\n",
    "                ## Step 6: get the predict result and then compute accuracy\n",
    "                ##---------------------------------------------------------\n",
    "                y_pred = torch.argmax(output_y.data, 1)\n",
    "                accy = _compute_counts(y_pred, y_labels)/batch_size\n",
    "                ##----------------------------------------------------------\n",
    "                ## Step 7: print loss values [I have done it]\n",
    "                ##----------------------------------------------------------\n",
    "                if iteration%10==0:\n",
    "                    print('iter: {} loss: {}, accy: {}'.format(iteration, loss.item(), accy))\n",
    "                    wandb.log({'iter': iteration, 'loss': loss.item()})\n",
    "                    wandb.log({'iter': iteration, 'accy': accy})\n",
    "                    \n",
    "            ##---------------------------------------------------\n",
    "            ##    save checkpoint below\n",
    "            ##---------------------------------------------------\n",
    "            _save_checkpoint(ckp_path, model, epoch, optimizer, iteration)\n",
    "    \n",
    "    ##------------------------------------\n",
    "    ##    model testing code below\n",
    "    ##------------------------------------\n",
    "    total = 0\n",
    "    accy_count = 0\n",
    "    model.eval() ##test\n",
    "    with torch.no_grad(): ## no gradient update\n",
    "        for batch_id, (x_batch,y_labels) in enumerate(test_loader):\n",
    "            x_batch, y_labels = Variable(x_batch).to(device), Variable(y_labels).to(device)\n",
    "            ##---------------------------------------\n",
    "            ## Step 8: write the predict result below\n",
    "            ##---------------------------------------\n",
    "            output_y = model(x_batch)\n",
    "            y_pred = torch.argmax(output_y.data, 1)\n",
    "            \n",
    "            ##--------------------------------------------------\n",
    "            ## Step 9: computing the test accuracy\n",
    "            ##---------------------------------------------------\n",
    "            total += len(y_labels)\n",
    "            accy_count += _compute_counts(y_pred, y_labels)\n",
    "    accy = accy_count/total\n",
    "    print(\"testing accy: \", accy)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b9a2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/huajieshao/Dropbox/WM-courses-Research/CSCI420-Data-Mining-Undergrad/Demo-code/Pytorch-demo/wandb/run-20230408_234000-5hy29qd5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uiuc-taylor/MLP/runs/5hy29qd5' target=\"_blank\">MLP_demo</a></strong> to <a href='https://wandb.ai/uiuc-taylor/MLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uiuc-taylor/MLP' target=\"_blank\">https://wandb.ai/uiuc-taylor/MLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uiuc-taylor/MLP/runs/5hy29qd5' target=\"_blank\">https://wandb.ai/uiuc-taylor/MLP/runs/5hy29qd5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "iter: 10 loss: 2.2467539310455322, accy: 0.16\n",
      "iter: 20 loss: 2.26986026763916, accy: 0.16\n",
      "iter: 30 loss: 2.2397360801696777, accy: 0.24\n",
      "iter: 40 loss: 2.248927593231201, accy: 0.22\n",
      "iter: 50 loss: 2.2449376583099365, accy: 0.2\n",
      "iter: 60 loss: 2.188424825668335, accy: 0.24\n",
      "iter: 70 loss: 2.1766555309295654, accy: 0.32\n",
      "iter: 80 loss: 2.1025702953338623, accy: 0.4\n",
      "iter: 90 loss: 2.14996337890625, accy: 0.26\n",
      "iter: 100 loss: 2.0548348426818848, accy: 0.38\n",
      "iter: 110 loss: 2.0163094997406006, accy: 0.4\n",
      "iter: 120 loss: 2.0130138397216797, accy: 0.42\n",
      "iter: 130 loss: 1.9852347373962402, accy: 0.42\n",
      "iter: 140 loss: 1.854904294013977, accy: 0.5\n",
      "iter: 150 loss: 1.8025190830230713, accy: 0.5\n",
      "iter: 160 loss: 1.8375836610794067, accy: 0.42\n",
      "iter: 170 loss: 1.8970354795455933, accy: 0.4\n",
      "iter: 180 loss: 1.7094659805297852, accy: 0.64\n",
      "iter: 190 loss: 1.7193937301635742, accy: 0.58\n",
      "iter: 200 loss: 1.8723993301391602, accy: 0.34\n",
      "iter: 210 loss: 1.589950442314148, accy: 0.66\n",
      "iter: 220 loss: 1.5335278511047363, accy: 0.52\n",
      "iter: 230 loss: 1.669425368309021, accy: 0.52\n",
      "iter: 240 loss: 1.5621391534805298, accy: 0.58\n",
      "iter: 250 loss: 1.5037178993225098, accy: 0.48\n",
      "iter: 260 loss: 1.562606692314148, accy: 0.58\n",
      "iter: 270 loss: 1.4229302406311035, accy: 0.56\n",
      "iter: 280 loss: 1.3887966871261597, accy: 0.6\n",
      "iter: 290 loss: 1.3442095518112183, accy: 0.68\n",
      "iter: 300 loss: 1.4333382844924927, accy: 0.62\n",
      "iter: 310 loss: 1.4879614114761353, accy: 0.58\n",
      "iter: 320 loss: 1.3330093622207642, accy: 0.66\n",
      "iter: 330 loss: 1.404258131980896, accy: 0.54\n",
      "iter: 340 loss: 1.400283694267273, accy: 0.52\n",
      "iter: 350 loss: 1.153747320175171, accy: 0.66\n",
      "iter: 360 loss: 1.1691718101501465, accy: 0.6\n",
      "iter: 370 loss: 1.3099762201309204, accy: 0.62\n",
      "iter: 380 loss: 1.307212233543396, accy: 0.56\n",
      "iter: 390 loss: 0.9796109199523926, accy: 0.78\n",
      "iter: 400 loss: 0.9660016894340515, accy: 0.74\n",
      "iter: 410 loss: 1.1110928058624268, accy: 0.7\n",
      "iter: 420 loss: 1.0859538316726685, accy: 0.6\n",
      "iter: 430 loss: 1.1572061777114868, accy: 0.56\n",
      "iter: 440 loss: 1.2955676317214966, accy: 0.58\n",
      "iter: 450 loss: 1.1284682750701904, accy: 0.72\n",
      "iter: 460 loss: 0.9637647867202759, accy: 0.76\n",
      "iter: 470 loss: 1.0471787452697754, accy: 0.66\n",
      "iter: 480 loss: 1.1789124011993408, accy: 0.52\n",
      "iter: 490 loss: 1.1476362943649292, accy: 0.6\n",
      "iter: 500 loss: 0.9790224432945251, accy: 0.76\n",
      "iter: 510 loss: 1.2452294826507568, accy: 0.54\n",
      "iter: 520 loss: 0.9884700179100037, accy: 0.72\n",
      "iter: 530 loss: 1.1874984502792358, accy: 0.68\n",
      "iter: 540 loss: 1.0843719244003296, accy: 0.54\n",
      "iter: 550 loss: 0.9360860586166382, accy: 0.7\n",
      "iter: 560 loss: 1.0093321800231934, accy: 0.66\n",
      "iter: 570 loss: 0.9560004472732544, accy: 0.74\n",
      "iter: 580 loss: 1.1256964206695557, accy: 0.68\n",
      "iter: 590 loss: 0.8320965766906738, accy: 0.74\n",
      "iter: 600 loss: 1.0907433032989502, accy: 0.7\n",
      "iter: 610 loss: 0.9513156414031982, accy: 0.68\n",
      "iter: 620 loss: 0.9581708312034607, accy: 0.66\n",
      "iter: 630 loss: 0.7915109992027283, accy: 0.76\n",
      "iter: 640 loss: 1.0118730068206787, accy: 0.72\n",
      "iter: 650 loss: 1.1321287155151367, accy: 0.52\n",
      "iter: 660 loss: 0.8661041855812073, accy: 0.7\n",
      "iter: 670 loss: 0.8503077030181885, accy: 0.72\n",
      "iter: 680 loss: 1.0615077018737793, accy: 0.62\n",
      "iter: 690 loss: 0.9714654684066772, accy: 0.68\n",
      "iter: 700 loss: 0.6214282512664795, accy: 0.84\n",
      "iter: 710 loss: 0.7741366028785706, accy: 0.84\n",
      "iter: 720 loss: 0.8546753525733948, accy: 0.74\n",
      "iter: 730 loss: 0.883719801902771, accy: 0.68\n",
      "iter: 740 loss: 0.9443425536155701, accy: 0.66\n",
      "iter: 750 loss: 0.9027165174484253, accy: 0.78\n",
      "iter: 760 loss: 1.0119092464447021, accy: 0.74\n",
      "iter: 770 loss: 0.8208897113800049, accy: 0.7\n",
      "iter: 780 loss: 0.9902171492576599, accy: 0.64\n",
      "iter: 790 loss: 1.0423575639724731, accy: 0.62\n",
      "iter: 800 loss: 0.8834190368652344, accy: 0.62\n",
      "iter: 810 loss: 0.6991832852363586, accy: 0.78\n",
      "iter: 820 loss: 0.7899335622787476, accy: 0.76\n",
      "iter: 830 loss: 0.8160337805747986, accy: 0.72\n",
      "iter: 840 loss: 0.8713681697845459, accy: 0.66\n",
      "iter: 850 loss: 0.9506409168243408, accy: 0.7\n",
      "iter: 860 loss: 0.7948550581932068, accy: 0.76\n",
      "iter: 870 loss: 0.9223738312721252, accy: 0.72\n",
      "iter: 880 loss: 0.8626157641410828, accy: 0.66\n",
      "iter: 890 loss: 0.7213795185089111, accy: 0.78\n",
      "iter: 900 loss: 0.7717116475105286, accy: 0.74\n",
      "iter: 910 loss: 0.7851728200912476, accy: 0.68\n",
      "iter: 920 loss: 0.9384496212005615, accy: 0.68\n",
      "iter: 930 loss: 0.7435992956161499, accy: 0.72\n",
      "iter: 940 loss: 0.8264179825782776, accy: 0.74\n",
      "iter: 950 loss: 0.9213449358940125, accy: 0.62\n",
      "iter: 960 loss: 0.8229718208312988, accy: 0.7\n",
      "iter: 970 loss: 0.9447715282440186, accy: 0.6\n",
      "iter: 980 loss: 0.9404693841934204, accy: 0.76\n",
      "iter: 990 loss: 0.7916110754013062, accy: 0.74\n",
      "iter: 1000 loss: 1.010033369064331, accy: 0.64\n",
      "iter: 1010 loss: 0.6092187762260437, accy: 0.82\n",
      "iter: 1020 loss: 0.7363540530204773, accy: 0.76\n",
      "iter: 1030 loss: 0.9942890405654907, accy: 0.64\n",
      "iter: 1040 loss: 0.9045221209526062, accy: 0.68\n",
      "iter: 1050 loss: 0.7583552598953247, accy: 0.74\n",
      "iter: 1060 loss: 0.7545376420021057, accy: 0.7\n",
      "iter: 1070 loss: 0.9781273007392883, accy: 0.66\n",
      "iter: 1080 loss: 0.5964972376823425, accy: 0.76\n",
      "iter: 1090 loss: 0.8048147559165955, accy: 0.8\n",
      "iter: 1100 loss: 0.5415899753570557, accy: 0.84\n",
      "iter: 1110 loss: 0.751720130443573, accy: 0.8\n",
      "iter: 1120 loss: 0.7623206973075867, accy: 0.74\n",
      "iter: 1130 loss: 0.5652281045913696, accy: 0.82\n",
      "iter: 1140 loss: 0.6533563137054443, accy: 0.8\n",
      "iter: 1150 loss: 0.8478579521179199, accy: 0.78\n",
      "iter: 1160 loss: 0.7785489559173584, accy: 0.78\n",
      "iter: 1170 loss: 0.5699284076690674, accy: 0.86\n",
      "iter: 1180 loss: 0.8403327465057373, accy: 0.76\n",
      "iter: 1190 loss: 0.6847904324531555, accy: 0.8\n",
      "iter: 1200 loss: 0.7273537516593933, accy: 0.76\n",
      "iter: 1210 loss: 0.7293158173561096, accy: 0.76\n",
      "iter: 1220 loss: 0.6519935727119446, accy: 0.8\n",
      "iter: 1230 loss: 0.8408216238021851, accy: 0.74\n",
      "iter: 1240 loss: 0.7788043022155762, accy: 0.72\n",
      "iter: 1250 loss: 0.7588313221931458, accy: 0.74\n",
      "iter: 1260 loss: 0.7038848996162415, accy: 0.82\n",
      "iter: 1270 loss: 0.7338646650314331, accy: 0.74\n",
      "iter: 1280 loss: 0.6859554052352905, accy: 0.82\n",
      "iter: 1290 loss: 0.730546236038208, accy: 0.74\n",
      "iter: 1300 loss: 0.7678797245025635, accy: 0.76\n",
      "iter: 1310 loss: 0.6404034495353699, accy: 0.84\n",
      "iter: 1320 loss: 0.9022691249847412, accy: 0.76\n",
      "iter: 1330 loss: 0.7727905511856079, accy: 0.74\n",
      "iter: 1340 loss: 1.132062315940857, accy: 0.6\n",
      "iter: 1350 loss: 0.7586609125137329, accy: 0.68\n",
      "iter: 1360 loss: 0.6010990142822266, accy: 0.82\n",
      "iter: 1370 loss: 0.6351797580718994, accy: 0.82\n",
      "iter: 1380 loss: 0.9142839908599854, accy: 0.74\n",
      "iter: 1390 loss: 0.7327345013618469, accy: 0.7\n",
      "iter: 1400 loss: 0.9245284199714661, accy: 0.72\n",
      "iter: 1410 loss: 1.018392562866211, accy: 0.7\n",
      "iter: 1420 loss: 0.6296401023864746, accy: 0.86\n",
      "iter: 1430 loss: 0.8176352977752686, accy: 0.7\n",
      "iter: 1440 loss: 0.6810630559921265, accy: 0.76\n",
      "iter: 1450 loss: 0.5727444291114807, accy: 0.78\n",
      "iter: 1460 loss: 0.6598451733589172, accy: 0.76\n",
      "iter: 1470 loss: 0.5841019153594971, accy: 0.76\n",
      "iter: 1480 loss: 0.5443454384803772, accy: 0.82\n",
      "iter: 1490 loss: 0.8724331855773926, accy: 0.78\n",
      "iter: 1500 loss: 0.549627423286438, accy: 0.84\n",
      "iter: 1510 loss: 0.7734917402267456, accy: 0.64\n",
      "iter: 1520 loss: 0.588721752166748, accy: 0.82\n",
      "iter: 1530 loss: 0.6947795152664185, accy: 0.76\n",
      "iter: 1540 loss: 0.6582057476043701, accy: 0.78\n",
      "iter: 1550 loss: 0.6208551526069641, accy: 0.78\n",
      "iter: 1560 loss: 0.7052429914474487, accy: 0.72\n",
      "iter: 1570 loss: 0.5435581803321838, accy: 0.8\n",
      "iter: 1580 loss: 0.6354151964187622, accy: 0.8\n",
      "iter: 1590 loss: 0.9899949431419373, accy: 0.66\n",
      "iter: 1600 loss: 0.6917375922203064, accy: 0.76\n",
      "iter: 1610 loss: 0.7594231963157654, accy: 0.68\n",
      "iter: 1620 loss: 0.8502359986305237, accy: 0.76\n",
      "iter: 1630 loss: 0.7522078156471252, accy: 0.74\n",
      "iter: 1640 loss: 0.7095063924789429, accy: 0.74\n",
      "iter: 1650 loss: 0.537794291973114, accy: 0.82\n",
      "iter: 1660 loss: 0.5704149603843689, accy: 0.84\n",
      "iter: 1670 loss: 0.8386992812156677, accy: 0.74\n",
      "iter: 1680 loss: 0.6915960907936096, accy: 0.74\n",
      "iter: 1690 loss: 0.514752209186554, accy: 0.86\n",
      "iter: 1700 loss: 0.8120420575141907, accy: 0.74\n",
      "iter: 1710 loss: 0.6831710338592529, accy: 0.76\n",
      "iter: 1720 loss: 0.6569125652313232, accy: 0.8\n",
      "iter: 1730 loss: 0.5271010994911194, accy: 0.82\n",
      "iter: 1740 loss: 0.5094856023788452, accy: 0.84\n",
      "iter: 1750 loss: 0.7931227684020996, accy: 0.82\n",
      "iter: 1760 loss: 0.493764728307724, accy: 0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1770 loss: 0.6567776203155518, accy: 0.74\n",
      "iter: 1780 loss: 0.6194265484809875, accy: 0.82\n",
      "iter: 1790 loss: 0.6990736126899719, accy: 0.76\n",
      "iter: 1800 loss: 0.6239172220230103, accy: 0.76\n",
      "iter: 1810 loss: 0.6141528487205505, accy: 0.82\n",
      "iter: 1820 loss: 0.6803627610206604, accy: 0.8\n",
      "iter: 1830 loss: 0.6330241560935974, accy: 0.82\n",
      "iter: 1840 loss: 0.6751653552055359, accy: 0.74\n",
      "iter: 1850 loss: 0.9091954231262207, accy: 0.74\n",
      "iter: 1860 loss: 0.7397760152816772, accy: 0.78\n",
      "iter: 1870 loss: 0.5643408298492432, accy: 0.76\n",
      "iter: 1880 loss: 0.6540694236755371, accy: 0.78\n",
      "iter: 1890 loss: 0.6912649273872375, accy: 0.72\n",
      "iter: 1900 loss: 0.8282075524330139, accy: 0.72\n",
      "iter: 1910 loss: 0.7204115986824036, accy: 0.74\n",
      "iter: 1920 loss: 0.9447862505912781, accy: 0.6\n",
      "iter: 1930 loss: 0.7248539924621582, accy: 0.84\n",
      "iter: 1940 loss: 0.8394641280174255, accy: 0.76\n",
      "iter: 1950 loss: 0.5565263628959656, accy: 0.82\n",
      "iter: 1960 loss: 0.653151273727417, accy: 0.78\n",
      "iter: 1970 loss: 0.666596531867981, accy: 0.78\n",
      "iter: 1980 loss: 0.6752442717552185, accy: 0.72\n",
      "iter: 1990 loss: 0.7544640898704529, accy: 0.78\n",
      "iter: 2000 loss: 0.6971631050109863, accy: 0.76\n",
      "iter: 2010 loss: 0.635001540184021, accy: 0.82\n",
      "iter: 2020 loss: 0.7217576503753662, accy: 0.76\n",
      "iter: 2030 loss: 0.7091518640518188, accy: 0.74\n",
      "iter: 2040 loss: 0.9603279829025269, accy: 0.7\n",
      "iter: 2050 loss: 0.6302071213722229, accy: 0.8\n",
      "iter: 2060 loss: 0.6882026791572571, accy: 0.72\n",
      "iter: 2070 loss: 0.641476035118103, accy: 0.76\n",
      "iter: 2080 loss: 0.6162176132202148, accy: 0.8\n",
      "iter: 2090 loss: 1.0279232263565063, accy: 0.72\n",
      "iter: 2100 loss: 0.6813493371009827, accy: 0.82\n",
      "iter: 2110 loss: 0.638938307762146, accy: 0.76\n",
      "iter: 2120 loss: 0.4223209619522095, accy: 0.9\n",
      "iter: 2130 loss: 0.6014705896377563, accy: 0.76\n",
      "iter: 2140 loss: 0.8378040790557861, accy: 0.7\n",
      "iter: 2150 loss: 0.7150251269340515, accy: 0.76\n",
      "iter: 2160 loss: 0.5272414684295654, accy: 0.82\n",
      "iter: 2170 loss: 0.6817963123321533, accy: 0.76\n",
      "iter: 2180 loss: 0.586585283279419, accy: 0.8\n",
      "iter: 2190 loss: 0.6682485342025757, accy: 0.8\n",
      "iter: 2200 loss: 0.5790683627128601, accy: 0.8\n",
      "iter: 2210 loss: 0.715971827507019, accy: 0.74\n",
      "iter: 2220 loss: 0.8961963057518005, accy: 0.74\n",
      "iter: 2230 loss: 0.5517635941505432, accy: 0.82\n",
      "iter: 2240 loss: 0.593306303024292, accy: 0.82\n",
      "iter: 2250 loss: 0.6101992130279541, accy: 0.84\n",
      "iter: 2260 loss: 0.3922891914844513, accy: 0.86\n",
      "iter: 2270 loss: 0.6913900971412659, accy: 0.78\n",
      "iter: 2280 loss: 0.7730796933174133, accy: 0.8\n",
      "iter: 2290 loss: 0.7044531106948853, accy: 0.8\n",
      "iter: 2300 loss: 0.5820453763008118, accy: 0.82\n",
      "iter: 2310 loss: 0.6637254357337952, accy: 0.78\n",
      "iter: 2320 loss: 0.5587999820709229, accy: 0.82\n",
      "iter: 2330 loss: 0.7392787337303162, accy: 0.7\n",
      "iter: 2340 loss: 0.8396772146224976, accy: 0.76\n",
      "iter: 2350 loss: 0.6468214392662048, accy: 0.78\n",
      "iter: 2360 loss: 0.6249638795852661, accy: 0.8\n",
      "iter: 2370 loss: 0.6602112054824829, accy: 0.82\n",
      "iter: 2380 loss: 0.6247108578681946, accy: 0.82\n",
      "iter: 2390 loss: 0.6080299615859985, accy: 0.8\n",
      "iter: 2400 loss: 0.423003226518631, accy: 0.86\n",
      "iter: 2410 loss: 0.9361071586608887, accy: 0.76\n",
      "iter: 2420 loss: 0.6502190232276917, accy: 0.82\n",
      "iter: 2430 loss: 0.5948947072029114, accy: 0.8\n",
      "iter: 2440 loss: 0.655119776725769, accy: 0.78\n",
      "iter: 2450 loss: 0.7591074109077454, accy: 0.8\n",
      "iter: 2460 loss: 0.663056492805481, accy: 0.72\n",
      "iter: 2470 loss: 0.6136608123779297, accy: 0.8\n",
      "iter: 2480 loss: 0.7149751782417297, accy: 0.76\n",
      "iter: 2490 loss: 0.39033767580986023, accy: 0.92\n",
      "iter: 2500 loss: 0.6513567566871643, accy: 0.82\n",
      "iter: 2510 loss: 0.8310468196868896, accy: 0.7\n",
      "iter: 2520 loss: 0.708751380443573, accy: 0.78\n",
      "iter: 2530 loss: 0.6128750443458557, accy: 0.8\n",
      "iter: 2540 loss: 0.4590226113796234, accy: 0.88\n",
      "iter: 2550 loss: 0.5376592874526978, accy: 0.78\n",
      "iter: 2560 loss: 0.7634081840515137, accy: 0.74\n",
      "iter: 2570 loss: 0.5555341243743896, accy: 0.86\n",
      "iter: 2580 loss: 0.6561210751533508, accy: 0.78\n",
      "iter: 2590 loss: 0.4446909725666046, accy: 0.9\n",
      "iter: 2600 loss: 0.7396870255470276, accy: 0.8\n",
      "iter: 2610 loss: 0.7201426029205322, accy: 0.72\n",
      "iter: 2620 loss: 0.6751587390899658, accy: 0.8\n",
      "iter: 2630 loss: 0.5501174330711365, accy: 0.8\n",
      "iter: 2640 loss: 0.4507249891757965, accy: 0.84\n",
      "iter: 2650 loss: 0.6474942564964294, accy: 0.8\n",
      "iter: 2660 loss: 0.5515133142471313, accy: 0.82\n",
      "iter: 2670 loss: 0.6140270829200745, accy: 0.8\n",
      "iter: 2680 loss: 0.6744492053985596, accy: 0.8\n",
      "iter: 2690 loss: 0.5855572819709778, accy: 0.8\n",
      "iter: 2700 loss: 0.6354393362998962, accy: 0.76\n",
      "iter: 2710 loss: 0.5689225196838379, accy: 0.86\n",
      "iter: 2720 loss: 0.505098819732666, accy: 0.82\n",
      "iter: 2730 loss: 0.8185403943061829, accy: 0.68\n",
      "iter: 2740 loss: 0.856278657913208, accy: 0.78\n",
      "iter: 2750 loss: 0.8175119161605835, accy: 0.78\n",
      "iter: 2760 loss: 0.4424130320549011, accy: 0.86\n",
      "iter: 2770 loss: 0.5434569120407104, accy: 0.82\n",
      "iter: 2780 loss: 0.6774613857269287, accy: 0.76\n",
      "iter: 2790 loss: 0.820098876953125, accy: 0.76\n",
      "iter: 2800 loss: 0.5433787703514099, accy: 0.8\n",
      "iter: 2810 loss: 0.7190473675727844, accy: 0.72\n",
      "iter: 2820 loss: 0.4924471974372864, accy: 0.86\n",
      "iter: 2830 loss: 0.5993606448173523, accy: 0.76\n",
      "iter: 2840 loss: 0.6280040144920349, accy: 0.76\n",
      "iter: 2850 loss: 0.9428555369377136, accy: 0.62\n",
      "iter: 2860 loss: 0.5189614295959473, accy: 0.84\n",
      "iter: 2870 loss: 0.69651198387146, accy: 0.76\n",
      "iter: 2880 loss: 0.5459294319152832, accy: 0.8\n",
      "iter: 2890 loss: 0.49326956272125244, accy: 0.84\n",
      "iter: 2900 loss: 0.6886751055717468, accy: 0.84\n",
      "iter: 2910 loss: 0.6856740713119507, accy: 0.78\n",
      "iter: 2920 loss: 0.41470733284950256, accy: 0.88\n",
      "iter: 2930 loss: 0.4280417263507843, accy: 0.9\n",
      "iter: 2940 loss: 0.6119700074195862, accy: 0.8\n",
      "iter: 2950 loss: 0.7758240699768066, accy: 0.74\n",
      "iter: 2960 loss: 0.7434133887290955, accy: 0.78\n",
      "iter: 2970 loss: 0.7039242386817932, accy: 0.7\n",
      "iter: 2980 loss: 0.5555270910263062, accy: 0.8\n",
      "iter: 2990 loss: 0.6264991164207458, accy: 0.76\n",
      "iter: 3000 loss: 0.7297849059104919, accy: 0.74\n",
      "iter: 3010 loss: 0.8232553005218506, accy: 0.66\n",
      "iter: 3020 loss: 0.5595685243606567, accy: 0.8\n",
      "iter: 3030 loss: 0.5583368539810181, accy: 0.78\n",
      "iter: 3040 loss: 0.4193970859050751, accy: 0.84\n",
      "iter: 3050 loss: 0.6372269988059998, accy: 0.76\n",
      "iter: 3060 loss: 0.7926849126815796, accy: 0.78\n",
      "iter: 3070 loss: 0.5476745367050171, accy: 0.86\n",
      "iter: 3080 loss: 0.7654296159744263, accy: 0.76\n",
      "iter: 3090 loss: 0.44138801097869873, accy: 0.82\n",
      "iter: 3100 loss: 0.5591139793395996, accy: 0.78\n",
      "iter: 3110 loss: 0.5572946071624756, accy: 0.8\n",
      "iter: 3120 loss: 0.5217962861061096, accy: 0.86\n",
      "iter: 3130 loss: 0.7000421285629272, accy: 0.78\n",
      "iter: 3140 loss: 0.719153642654419, accy: 0.72\n",
      "iter: 3150 loss: 0.47016823291778564, accy: 0.86\n",
      "iter: 3160 loss: 0.5237529277801514, accy: 0.8\n",
      "iter: 3170 loss: 0.468148410320282, accy: 0.82\n",
      "iter: 3180 loss: 0.7680051922798157, accy: 0.68\n",
      "iter: 3190 loss: 0.4199053943157196, accy: 0.88\n",
      "iter: 3200 loss: 0.5715817809104919, accy: 0.84\n",
      "iter: 3210 loss: 0.710822343826294, accy: 0.76\n",
      "iter: 3220 loss: 0.5722506642341614, accy: 0.84\n",
      "iter: 3230 loss: 0.6220848560333252, accy: 0.86\n",
      "iter: 3240 loss: 0.8157517910003662, accy: 0.7\n",
      "iter: 3250 loss: 0.7318071126937866, accy: 0.76\n",
      "iter: 3260 loss: 0.7961257696151733, accy: 0.74\n",
      "iter: 3270 loss: 0.42546531558036804, accy: 0.86\n",
      "iter: 3280 loss: 0.7722766399383545, accy: 0.74\n",
      "iter: 3290 loss: 0.48905178904533386, accy: 0.86\n",
      "iter: 3300 loss: 0.32895106077194214, accy: 0.9\n",
      "iter: 3310 loss: 0.47963523864746094, accy: 0.86\n",
      "iter: 3320 loss: 0.5782625079154968, accy: 0.78\n",
      "iter: 3330 loss: 0.5604572892189026, accy: 0.72\n",
      "iter: 3340 loss: 0.557110071182251, accy: 0.88\n",
      "iter: 3350 loss: 0.5922227501869202, accy: 0.8\n",
      "iter: 3360 loss: 0.49505096673965454, accy: 0.82\n",
      "iter: 3370 loss: 0.7436394691467285, accy: 0.76\n",
      "iter: 3380 loss: 0.3623603880405426, accy: 0.92\n",
      "iter: 3390 loss: 0.6501886248588562, accy: 0.8\n",
      "iter: 3400 loss: 0.46712052822113037, accy: 0.82\n",
      "iter: 3410 loss: 0.4663730561733246, accy: 0.84\n",
      "iter: 3420 loss: 0.582793116569519, accy: 0.78\n",
      "iter: 3430 loss: 0.4566417634487152, accy: 0.82\n",
      "iter: 3440 loss: 0.4489017426967621, accy: 0.88\n",
      "iter: 3450 loss: 0.6893053650856018, accy: 0.84\n",
      "iter: 3460 loss: 0.7068418860435486, accy: 0.74\n",
      "iter: 3470 loss: 0.7335205674171448, accy: 0.74\n",
      "iter: 3480 loss: 0.5739849805831909, accy: 0.82\n",
      "iter: 3490 loss: 0.617101788520813, accy: 0.78\n",
      "iter: 3500 loss: 0.34899288415908813, accy: 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 3510 loss: 0.4595865309238434, accy: 0.9\n",
      "iter: 3520 loss: 0.5014176368713379, accy: 0.82\n",
      "iter: 3530 loss: 0.835774302482605, accy: 0.76\n",
      "iter: 3540 loss: 0.4900963604450226, accy: 0.86\n",
      "iter: 3550 loss: 0.7871101498603821, accy: 0.76\n",
      "iter: 3560 loss: 0.4468817412853241, accy: 0.9\n",
      "iter: 3570 loss: 0.8099664449691772, accy: 0.74\n",
      "iter: 3580 loss: 0.53631192445755, accy: 0.82\n",
      "iter: 3590 loss: 0.528367817401886, accy: 0.86\n",
      "iter: 3600 loss: 0.7351015210151672, accy: 0.76\n",
      "iter: 3610 loss: 0.6437206864356995, accy: 0.84\n",
      "iter: 3620 loss: 0.41012537479400635, accy: 0.84\n",
      "iter: 3630 loss: 0.6580483913421631, accy: 0.82\n",
      "iter: 3640 loss: 0.5768964886665344, accy: 0.86\n",
      "iter: 3650 loss: 0.5376125574111938, accy: 0.78\n",
      "iter: 3660 loss: 0.6230846643447876, accy: 0.82\n",
      "iter: 3670 loss: 0.6075738668441772, accy: 0.88\n",
      "iter: 3680 loss: 0.5069350004196167, accy: 0.84\n",
      "iter: 3690 loss: 0.6758291721343994, accy: 0.8\n",
      "iter: 3700 loss: 0.46932682394981384, accy: 0.86\n",
      "iter: 3710 loss: 0.625130295753479, accy: 0.86\n",
      "iter: 3720 loss: 0.44288185238838196, accy: 0.88\n",
      "iter: 3730 loss: 0.4051891565322876, accy: 0.88\n",
      "iter: 3740 loss: 0.45088160037994385, accy: 0.8\n",
      "iter: 3750 loss: 0.3999677896499634, accy: 0.92\n",
      "iter: 3760 loss: 0.4252070486545563, accy: 0.82\n",
      "iter: 3770 loss: 0.7367227673530579, accy: 0.78\n",
      "iter: 3780 loss: 0.8424445986747742, accy: 0.74\n",
      "iter: 3790 loss: 0.7577323317527771, accy: 0.86\n",
      "iter: 3800 loss: 0.5479806065559387, accy: 0.84\n",
      "iter: 3810 loss: 0.5344177484512329, accy: 0.82\n",
      "iter: 3820 loss: 0.6792842149734497, accy: 0.76\n",
      "iter: 3830 loss: 0.7902490496635437, accy: 0.8\n",
      "iter: 3840 loss: 0.54079669713974, accy: 0.82\n",
      "iter: 3850 loss: 0.5479450821876526, accy: 0.76\n",
      "iter: 3860 loss: 0.5678766369819641, accy: 0.78\n",
      "iter: 3870 loss: 0.44668424129486084, accy: 0.84\n",
      "iter: 3880 loss: 0.4848114252090454, accy: 0.84\n",
      "iter: 3890 loss: 0.7408691644668579, accy: 0.76\n",
      "iter: 3900 loss: 0.3464187979698181, accy: 0.88\n",
      "iter: 3910 loss: 0.6443280577659607, accy: 0.72\n",
      "iter: 3920 loss: 0.5181785821914673, accy: 0.78\n",
      "iter: 3930 loss: 0.6638234853744507, accy: 0.8\n",
      "iter: 3940 loss: 0.5531144142150879, accy: 0.72\n",
      "iter: 3950 loss: 0.44901978969573975, accy: 0.8\n",
      "iter: 3960 loss: 0.8693283796310425, accy: 0.68\n",
      "iter: 3970 loss: 0.5908819437026978, accy: 0.8\n",
      "iter: 3980 loss: 0.5734492540359497, accy: 0.82\n",
      "iter: 3990 loss: 0.6227166652679443, accy: 0.78\n",
      "iter: 4000 loss: 0.4370166063308716, accy: 0.8\n",
      "iter: 4010 loss: 0.5303208231925964, accy: 0.84\n",
      "iter: 4020 loss: 0.46831217408180237, accy: 0.84\n",
      "iter: 4030 loss: 0.5533108711242676, accy: 0.82\n",
      "iter: 4040 loss: 0.5862547159194946, accy: 0.8\n",
      "iter: 4050 loss: 0.6028676629066467, accy: 0.74\n",
      "iter: 4060 loss: 0.5608888268470764, accy: 0.86\n",
      "iter: 4070 loss: 0.47148361802101135, accy: 0.9\n",
      "iter: 4080 loss: 0.493926465511322, accy: 0.84\n",
      "iter: 4090 loss: 0.2770179510116577, accy: 0.88\n",
      "iter: 4100 loss: 0.768808126449585, accy: 0.82\n",
      "iter: 4110 loss: 0.46810588240623474, accy: 0.84\n",
      "iter: 4120 loss: 0.563854992389679, accy: 0.88\n",
      "iter: 4130 loss: 0.3933815360069275, accy: 0.88\n",
      "iter: 4140 loss: 0.5659679174423218, accy: 0.86\n",
      "iter: 4150 loss: 0.42521166801452637, accy: 0.86\n",
      "iter: 4160 loss: 0.4937008023262024, accy: 0.82\n",
      "iter: 4170 loss: 0.7410586476325989, accy: 0.82\n",
      "iter: 4180 loss: 0.8063393235206604, accy: 0.74\n",
      "iter: 4190 loss: 0.42768368124961853, accy: 0.9\n",
      "iter: 4200 loss: 0.6817408204078674, accy: 0.8\n",
      "iter: 4210 loss: 0.5563040375709534, accy: 0.76\n",
      "iter: 4220 loss: 0.5300178527832031, accy: 0.82\n",
      "iter: 4230 loss: 0.5084398984909058, accy: 0.78\n",
      "iter: 4240 loss: 0.5792246460914612, accy: 0.84\n",
      "iter: 4250 loss: 0.6510362029075623, accy: 0.78\n",
      "iter: 4260 loss: 0.5938038229942322, accy: 0.86\n",
      "iter: 4270 loss: 0.3181127905845642, accy: 0.92\n",
      "iter: 4280 loss: 0.4490361511707306, accy: 0.86\n",
      "iter: 4290 loss: 0.38092368841171265, accy: 0.84\n",
      "iter: 4300 loss: 0.5326992273330688, accy: 0.86\n",
      "iter: 4310 loss: 0.5323765277862549, accy: 0.8\n",
      "iter: 4320 loss: 0.683942973613739, accy: 0.76\n",
      "iter: 4330 loss: 0.781912624835968, accy: 0.78\n",
      "iter: 4340 loss: 0.5222035646438599, accy: 0.78\n",
      "iter: 4350 loss: 0.6649104356765747, accy: 0.76\n",
      "iter: 4360 loss: 0.6565473079681396, accy: 0.74\n",
      "iter: 4370 loss: 0.40647247433662415, accy: 0.88\n",
      "iter: 4380 loss: 0.5477598905563354, accy: 0.84\n",
      "iter: 4390 loss: 0.7004036903381348, accy: 0.72\n",
      "iter: 4400 loss: 0.3204212188720703, accy: 0.9\n",
      "iter: 4410 loss: 0.4584650695323944, accy: 0.88\n",
      "iter: 4420 loss: 0.6885642409324646, accy: 0.8\n",
      "iter: 4430 loss: 0.3893791139125824, accy: 0.88\n",
      "iter: 4440 loss: 0.4437076449394226, accy: 0.82\n",
      "iter: 4450 loss: 0.7869107127189636, accy: 0.76\n",
      "iter: 4460 loss: 0.35393112897872925, accy: 0.88\n",
      "iter: 4470 loss: 0.46670231223106384, accy: 0.84\n",
      "iter: 4480 loss: 0.3386755883693695, accy: 0.92\n",
      "iter: 4490 loss: 0.48185718059539795, accy: 0.82\n",
      "iter: 4500 loss: 0.5605354905128479, accy: 0.86\n",
      "iter: 4510 loss: 0.39225760102272034, accy: 0.9\n",
      "iter: 4520 loss: 0.4773973524570465, accy: 0.84\n",
      "iter: 4530 loss: 0.5933288931846619, accy: 0.76\n",
      "iter: 4540 loss: 0.661517322063446, accy: 0.76\n",
      "iter: 4550 loss: 0.6020045280456543, accy: 0.84\n",
      "iter: 4560 loss: 0.5319608449935913, accy: 0.86\n",
      "iter: 4570 loss: 0.4674210250377655, accy: 0.84\n",
      "iter: 4580 loss: 0.7136009335517883, accy: 0.76\n",
      "iter: 4590 loss: 0.7143860459327698, accy: 0.8\n",
      "iter: 4600 loss: 0.6317964196205139, accy: 0.82\n",
      "iter: 4610 loss: 0.751290500164032, accy: 0.86\n",
      "iter: 4620 loss: 0.6668960452079773, accy: 0.78\n",
      "iter: 4630 loss: 0.4848291277885437, accy: 0.88\n",
      "iter: 4640 loss: 0.6578009724617004, accy: 0.74\n",
      "iter: 4650 loss: 0.8446412682533264, accy: 0.74\n",
      "iter: 4660 loss: 0.3103979825973511, accy: 0.92\n",
      "iter: 4670 loss: 0.509000301361084, accy: 0.86\n",
      "iter: 4680 loss: 0.6918871998786926, accy: 0.76\n",
      "iter: 4690 loss: 0.42815566062927246, accy: 0.82\n",
      "iter: 4700 loss: 0.3465510308742523, accy: 0.9\n",
      "iter: 4710 loss: 0.3685455024242401, accy: 0.88\n",
      "iter: 4720 loss: 0.3245963156223297, accy: 0.88\n",
      "iter: 4730 loss: 0.42287880182266235, accy: 0.9\n",
      "iter: 4740 loss: 0.5055763125419617, accy: 0.86\n",
      "iter: 4750 loss: 0.8789693713188171, accy: 0.78\n",
      "iter: 4760 loss: 0.5943572521209717, accy: 0.76\n",
      "iter: 4770 loss: 0.575705349445343, accy: 0.8\n",
      "iter: 4780 loss: 0.6738536357879639, accy: 0.76\n",
      "iter: 4790 loss: 0.5323565602302551, accy: 0.86\n",
      "iter: 4800 loss: 0.4396038055419922, accy: 0.86\n",
      "iter: 4810 loss: 0.47770655155181885, accy: 0.82\n",
      "iter: 4820 loss: 0.3852219879627228, accy: 0.88\n",
      "iter: 4830 loss: 0.5124537944793701, accy: 0.84\n",
      "iter: 4840 loss: 0.473537415266037, accy: 0.8\n",
      "iter: 4850 loss: 0.4883045256137848, accy: 0.86\n",
      "iter: 4860 loss: 0.5529040694236755, accy: 0.84\n",
      "iter: 4870 loss: 0.41509395837783813, accy: 0.88\n",
      "iter: 4880 loss: 0.7340306639671326, accy: 0.8\n",
      "iter: 4890 loss: 0.38801997900009155, accy: 0.84\n",
      "iter: 4900 loss: 0.4999578595161438, accy: 0.84\n",
      "iter: 4910 loss: 0.544914960861206, accy: 0.82\n",
      "iter: 4920 loss: 0.4738750457763672, accy: 0.82\n",
      "iter: 4930 loss: 0.38022032380104065, accy: 0.92\n",
      "iter: 4940 loss: 0.6813400983810425, accy: 0.76\n",
      "iter: 4950 loss: 0.3417031466960907, accy: 0.9\n",
      "iter: 4960 loss: 0.9085507988929749, accy: 0.72\n",
      "iter: 4970 loss: 0.6676551699638367, accy: 0.8\n",
      "iter: 4980 loss: 0.4434099495410919, accy: 0.86\n",
      "iter: 4990 loss: 0.4264931380748749, accy: 0.84\n",
      "iter: 5000 loss: 0.5447625517845154, accy: 0.82\n",
      "iter: 5010 loss: 0.46378767490386963, accy: 0.86\n",
      "iter: 5020 loss: 0.5441100001335144, accy: 0.78\n",
      "iter: 5030 loss: 0.629680871963501, accy: 0.82\n",
      "iter: 5040 loss: 0.5762118101119995, accy: 0.84\n",
      "iter: 5050 loss: 0.3929576873779297, accy: 0.88\n",
      "iter: 5060 loss: 0.38155463337898254, accy: 0.9\n",
      "iter: 5070 loss: 0.48669132590293884, accy: 0.86\n",
      "iter: 5080 loss: 0.45693156123161316, accy: 0.82\n",
      "iter: 5090 loss: 0.5218735337257385, accy: 0.88\n",
      "iter: 5100 loss: 0.3882251977920532, accy: 0.88\n",
      "iter: 5110 loss: 0.5202556848526001, accy: 0.88\n",
      "iter: 5120 loss: 0.23033934831619263, accy: 0.94\n",
      "iter: 5130 loss: 0.48279112577438354, accy: 0.82\n",
      "iter: 5140 loss: 0.4496895968914032, accy: 0.92\n",
      "iter: 5150 loss: 0.5629022121429443, accy: 0.82\n",
      "iter: 5160 loss: 0.49133002758026123, accy: 0.82\n",
      "iter: 5170 loss: 0.561153769493103, accy: 0.78\n",
      "iter: 5180 loss: 0.5188016891479492, accy: 0.84\n",
      "iter: 5190 loss: 0.6931913495063782, accy: 0.74\n",
      "iter: 5200 loss: 0.42445260286331177, accy: 0.88\n",
      "iter: 5210 loss: 0.5429553985595703, accy: 0.8\n",
      "iter: 5220 loss: 0.45738697052001953, accy: 0.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 5230 loss: 0.5712704658508301, accy: 0.8\n",
      "iter: 5240 loss: 0.58003830909729, accy: 0.78\n",
      "iter: 5250 loss: 0.5547457933425903, accy: 0.86\n",
      "iter: 5260 loss: 0.3831980228424072, accy: 0.9\n",
      "iter: 5270 loss: 0.42537567019462585, accy: 0.8\n",
      "iter: 5280 loss: 0.5064650177955627, accy: 0.86\n",
      "iter: 5290 loss: 0.4596623182296753, accy: 0.84\n",
      "iter: 5300 loss: 0.43958580493927, accy: 0.8\n",
      "iter: 5310 loss: 0.38412028551101685, accy: 0.86\n",
      "iter: 5320 loss: 0.5923264026641846, accy: 0.78\n",
      "iter: 5330 loss: 0.5382394790649414, accy: 0.82\n",
      "iter: 5340 loss: 0.4790576696395874, accy: 0.84\n",
      "iter: 5350 loss: 0.4102603495121002, accy: 0.9\n",
      "iter: 5360 loss: 0.42622727155685425, accy: 0.84\n",
      "iter: 5370 loss: 0.4234161078929901, accy: 0.9\n",
      "iter: 5380 loss: 0.4767766296863556, accy: 0.82\n",
      "iter: 5390 loss: 0.3377172350883484, accy: 0.92\n",
      "iter: 5400 loss: 0.6707130670547485, accy: 0.8\n",
      "iter: 5410 loss: 0.3497048318386078, accy: 0.9\n",
      "iter: 5420 loss: 0.6402935981750488, accy: 0.8\n",
      "iter: 5430 loss: 0.5620410442352295, accy: 0.78\n",
      "iter: 5440 loss: 0.49096593260765076, accy: 0.84\n",
      "iter: 5450 loss: 0.49378424882888794, accy: 0.9\n",
      "iter: 5460 loss: 0.538051962852478, accy: 0.84\n",
      "iter: 5470 loss: 0.7464953064918518, accy: 0.74\n",
      "iter: 5480 loss: 0.5789645910263062, accy: 0.84\n",
      "iter: 5490 loss: 0.6931290626525879, accy: 0.86\n",
      "iter: 5500 loss: 0.35784798860549927, accy: 0.9\n",
      "iter: 5510 loss: 0.6356161832809448, accy: 0.74\n",
      "iter: 5520 loss: 0.39862990379333496, accy: 0.82\n",
      "iter: 5530 loss: 0.343760222196579, accy: 0.9\n",
      "iter: 5540 loss: 0.5900305509567261, accy: 0.78\n",
      "iter: 5550 loss: 0.37225377559661865, accy: 0.88\n",
      "iter: 5560 loss: 0.44387730956077576, accy: 0.86\n",
      "iter: 5570 loss: 0.8547242879867554, accy: 0.7\n",
      "iter: 5580 loss: 0.4009857177734375, accy: 0.88\n",
      "iter: 5590 loss: 0.571337103843689, accy: 0.78\n",
      "iter: 5600 loss: 0.5790186524391174, accy: 0.82\n",
      "iter: 5610 loss: 0.4257639944553375, accy: 0.86\n",
      "iter: 5620 loss: 0.6448091864585876, accy: 0.82\n",
      "iter: 5630 loss: 0.4375448524951935, accy: 0.86\n",
      "iter: 5640 loss: 0.5727106332778931, accy: 0.8\n",
      "iter: 5650 loss: 0.4200950860977173, accy: 0.92\n",
      "iter: 5660 loss: 0.5406503081321716, accy: 0.78\n",
      "iter: 5670 loss: 0.41065025329589844, accy: 0.84\n",
      "iter: 5680 loss: 0.4900813698768616, accy: 0.86\n",
      "iter: 5690 loss: 0.3123117685317993, accy: 0.9\n",
      "iter: 5700 loss: 0.4621891379356384, accy: 0.86\n",
      "iter: 5710 loss: 0.414806604385376, accy: 0.84\n",
      "iter: 5720 loss: 0.6706511974334717, accy: 0.74\n",
      "iter: 5730 loss: 0.3721970021724701, accy: 0.88\n",
      "iter: 5740 loss: 0.40680381655693054, accy: 0.82\n",
      "iter: 5750 loss: 0.39569392800331116, accy: 0.84\n",
      "iter: 5760 loss: 0.34860336780548096, accy: 0.92\n",
      "iter: 5770 loss: 0.6196315884590149, accy: 0.86\n",
      "iter: 5780 loss: 0.3670841157436371, accy: 0.88\n",
      "iter: 5790 loss: 0.6637325286865234, accy: 0.8\n",
      "iter: 5800 loss: 0.42599689960479736, accy: 0.84\n",
      "iter: 5810 loss: 0.4931638240814209, accy: 0.84\n",
      "iter: 5820 loss: 0.7411441206932068, accy: 0.8\n",
      "iter: 5830 loss: 0.426030695438385, accy: 0.8\n",
      "iter: 5840 loss: 0.45039597153663635, accy: 0.86\n",
      "iter: 5850 loss: 0.6972383856773376, accy: 0.84\n",
      "iter: 5860 loss: 0.5027284622192383, accy: 0.84\n",
      "iter: 5870 loss: 0.5350486040115356, accy: 0.84\n",
      "iter: 5880 loss: 0.5683993697166443, accy: 0.82\n",
      "iter: 5890 loss: 0.735443115234375, accy: 0.76\n",
      "iter: 5900 loss: 0.6020556688308716, accy: 0.78\n",
      "iter: 5910 loss: 0.4302786886692047, accy: 0.82\n",
      "iter: 5920 loss: 0.725583016872406, accy: 0.76\n",
      "iter: 5930 loss: 0.5133359432220459, accy: 0.78\n",
      "iter: 5940 loss: 0.4318479597568512, accy: 0.92\n",
      "iter: 5950 loss: 0.5510521531105042, accy: 0.78\n",
      "iter: 5960 loss: 0.3349251449108124, accy: 0.9\n",
      "iter: 5970 loss: 0.3818384110927582, accy: 0.88\n",
      "iter: 5980 loss: 0.45851898193359375, accy: 0.82\n",
      "iter: 5990 loss: 0.4650965929031372, accy: 0.86\n",
      "iter: 6000 loss: 0.5555040836334229, accy: 0.76\n",
      "iter: 6010 loss: 0.311589777469635, accy: 0.86\n",
      "iter: 6020 loss: 0.5352321863174438, accy: 0.8\n",
      "iter: 6030 loss: 0.4351779818534851, accy: 0.82\n",
      "iter: 6040 loss: 0.39141446352005005, accy: 0.92\n",
      "iter: 6050 loss: 0.3471827208995819, accy: 0.88\n",
      "iter: 6060 loss: 0.3505997955799103, accy: 0.88\n",
      "iter: 6070 loss: 0.39036425948143005, accy: 0.88\n",
      "iter: 6080 loss: 0.3901067078113556, accy: 0.88\n",
      "iter: 6090 loss: 0.5435345768928528, accy: 0.84\n",
      "iter: 6100 loss: 0.4356844425201416, accy: 0.86\n",
      "iter: 6110 loss: 0.5573551058769226, accy: 0.8\n",
      "iter: 6120 loss: 0.438988596200943, accy: 0.86\n",
      "iter: 6130 loss: 0.3195204436779022, accy: 0.92\n",
      "iter: 6140 loss: 0.5001668930053711, accy: 0.84\n",
      "iter: 6150 loss: 0.26680439710617065, accy: 0.94\n",
      "iter: 6160 loss: 0.3965127468109131, accy: 0.88\n",
      "iter: 6170 loss: 0.5103132724761963, accy: 0.82\n",
      "iter: 6180 loss: 0.5694063305854797, accy: 0.78\n",
      "iter: 6190 loss: 0.296541690826416, accy: 0.86\n",
      "iter: 6200 loss: 0.3087897002696991, accy: 0.94\n",
      "iter: 6210 loss: 0.4246581196784973, accy: 0.88\n",
      "iter: 6220 loss: 0.4650459587574005, accy: 0.82\n",
      "iter: 6230 loss: 0.6026643514633179, accy: 0.8\n"
     ]
    }
   ],
   "source": [
    "with wandb.init(project='MLP', name='MLP_demo'):\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc335432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e0d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb832c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
